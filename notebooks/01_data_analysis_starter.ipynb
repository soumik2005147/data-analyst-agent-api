{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066874b3",
   "metadata": {},
   "source": [
    "# Data Analysis Starter Notebook\n",
    "\n",
    "This notebook provides a comprehensive template for data analysis projects. It includes data loading, exploration, visualization, and basic machine learning workflows.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Install Required Packages](#install)\n",
    "2. [Import Libraries](#imports)\n",
    "3. [Load and Explore Data](#load-data)\n",
    "4. [Data Cleaning and Preprocessing](#preprocessing)\n",
    "5. [Exploratory Data Analysis](#eda)\n",
    "6. [Feature Engineering](#feature-engineering)\n",
    "7. [Machine Learning Modeling](#modeling)\n",
    "8. [Results and Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d1006",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages {#install}\n",
    "\n",
    "Install essential Python packages and dependencies. Run this cell if packages are not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy matplotlib seaborn plotly scikit-learn jupyter\n",
    "\n",
    "# For additional packages\n",
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ba870",
   "metadata": {},
   "source": [
    "## 2. Import Libraries {#imports}\n",
    "\n",
    "Import all necessary libraries for data analysis and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf44545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "\n",
    "# System and utilities\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Custom modules (add src to path)\n",
    "sys.path.append('../src')\n",
    "from data.loader import load_csv_data, basic_data_info\n",
    "from visualization.plots import plot_distribution, correlation_heatmap\n",
    "from features.engineering import create_datetime_features, encode_categorical_features\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef74455",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Data {#load-data}\n",
    "\n",
    "Load your dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccc949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "DATA_PATH = Path('../data')\n",
    "RAW_DATA_PATH = DATA_PATH / 'raw'\n",
    "PROCESSED_DATA_PATH = DATA_PATH / 'processed'\n",
    "\n",
    "# Create sample dataset if none exists\n",
    "if not any(RAW_DATA_PATH.glob('*.csv')):\n",
    "    print(\"No CSV files found in raw data directory.\")\n",
    "    print(\"Creating sample dataset for demonstration...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    np.random.seed(42)\n",
    "    sample_data = {\n",
    "        'date': pd.date_range('2023-01-01', periods=1000, freq='D'),\n",
    "        'category': np.random.choice(['A', 'B', 'C'], 1000),\n",
    "        'value1': np.random.normal(50, 15, 1000),\n",
    "        'value2': np.random.normal(100, 25, 1000),\n",
    "        'target': np.random.randint(0, 2, 1000)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df.to_csv(RAW_DATA_PATH / 'sample_data.csv', index=False)\n",
    "    print(f\"Sample dataset saved to {RAW_DATA_PATH / 'sample_data.csv'}\")\n",
    "else:\n",
    "    # Load existing data\n",
    "    data_file = list(RAW_DATA_PATH.glob('*.csv'))[0]\n",
    "    df = load_csv_data(data_file)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eddcf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data information\n",
    "info = basic_data_info(df)\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Shape: {info['shape']}\")\n",
    "print(f\"Columns: {info['columns']}\")\n",
    "print(f\"\\nData Types:\")\n",
    "for col, dtype in info['dtypes'].items():\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "\n",
    "print(f\"\\nMissing Values:\")\n",
    "missing_data = {k: v for k, v in info['missing_values'].items() if v > 0}\n",
    "if missing_data:\n",
    "    for col, missing_count in missing_data.items():\n",
    "        print(f\"  {col}: {missing_count}\")\n",
    "else:\n",
    "    print(\"  No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2729ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe50081",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preprocessing {#preprocessing}\n",
    "\n",
    "Clean and preprocess the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedfb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning steps\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "# Example: Fill numeric columns with median, categorical with mode\n",
    "numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
    "categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if df_clean[col].isnull().any():\n",
    "        df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if df_clean[col].isnull().any():\n",
    "        df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "print(f\"Removing {df_clean.duplicated().sum()} duplicate rows...\")\n",
    "df_clean.drop_duplicates(inplace=True)\n",
    "\n",
    "# Data type conversions\n",
    "if 'date' in df_clean.columns:\n",
    "    df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "print(\"Data cleaning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6f313",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis {#eda}\n",
    "\n",
    "Explore the data through visualizations and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c37d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for numeric columns\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    for col in numeric_cols[:3]:  # Plot first 3 numeric columns\n",
    "        plot_distribution(df_clean, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "if len(numeric_cols) > 1:\n",
    "    correlation_heatmap(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variable analysis\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    for col in categorical_cols[:2]:  # Plot first 2 categorical columns\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        value_counts = df_clean[col].value_counts()\n",
    "        value_counts.plot(kind='bar')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive visualization with Plotly\n",
    "if len(numeric_cols) >= 2:\n",
    "    col1, col2 = numeric_cols[0], numeric_cols[1]\n",
    "    color_col = categorical_cols[0] if len(categorical_cols) > 0 else None\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        df_clean, \n",
    "        x=col1, \n",
    "        y=col2, \n",
    "        color=color_col,\n",
    "        title=f'{col2} vs {col1}',\n",
    "        hover_data=numeric_cols.tolist()\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41922b46",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering {#feature-engineering}\n",
    "\n",
    "Create new features and prepare data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df_features = df_clean.copy()\n",
    "\n",
    "# Create datetime features if date column exists\n",
    "if 'date' in df_features.columns:\n",
    "    df_features = create_datetime_features(df_features, 'date', drop_original=False)\n",
    "    print(\"Created datetime features\")\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = df_features.select_dtypes(include=['object']).columns\n",
    "categorical_cols = [col for col in categorical_cols if col != 'date']  # Exclude date column\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    df_features = encode_categorical_features(df_features, categorical_cols, method='onehot')\n",
    "    print(f\"Encoded categorical features: {categorical_cols}\")\n",
    "\n",
    "# Create interaction features (example)\n",
    "numeric_cols = df_features.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_cols) >= 2:\n",
    "    col1, col2 = numeric_cols[0], numeric_cols[1]\n",
    "    df_features[f'{col1}_{col2}_interaction'] = df_features[col1] * df_features[col2]\n",
    "    print(f\"Created interaction feature: {col1}_{col2}_interaction\")\n",
    "\n",
    "print(f\"\\nFeature engineering completed. New shape: {df_features.shape}\")\n",
    "print(f\"New columns: {list(set(df_features.columns) - set(df_clean.columns))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce8b82",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Modeling {#modeling}\n",
    "\n",
    "Build and evaluate machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e939118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# Assuming 'target' column exists, otherwise create a sample target\n",
    "if 'target' not in df_features.columns:\n",
    "    # Create a sample target variable\n",
    "    numeric_col = df_features.select_dtypes(include=[np.number]).columns[0]\n",
    "    df_features['target'] = (df_features[numeric_col] > df_features[numeric_col].median()).astype(int)\n",
    "    print(\"Created sample binary target variable\")\n",
    "\n",
    "# Select features and target\n",
    "feature_columns = [col for col in df_features.columns if col not in ['target', 'date']]\n",
    "X = df_features[feature_columns]\n",
    "y = df_features['target']\n",
    "\n",
    "# Handle any remaining non-numeric columns\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad438054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613786fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    if name == 'Logistic Regression':\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a55708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Test Accuracy': [results[model]['accuracy'] for model in results],\n",
    "    'CV Mean': [results[model]['cv_mean'] for model in results],\n",
    "    'CV Std': [results[model]['cv_std'] for model in results]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(model_comparison)\n",
    "\n",
    "# Visualize model performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "model_comparison.plot(x='Model', y='Test Accuracy', kind='bar', ax=ax1, color='skyblue')\n",
    "ax1.set_title('Model Test Accuracy Comparison')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Cross-validation scores\n",
    "ax2.errorbar(model_comparison['Model'], model_comparison['CV Mean'], \n",
    "             yerr=model_comparison['CV Std'], capsize=5, marker='o', linewidth=2)\n",
    "ax2.set_title('Cross-Validation Scores')\n",
    "ax2.set_ylabel('CV Score')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1499439",
   "metadata": {},
   "source": [
    "## 8. Results and Conclusions {#conclusions}\n",
    "\n",
    "Summarize findings and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9516df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for Random Forest)\n",
    "if 'Random Forest' in results:\n",
    "    rf_model = models['Random Forest']\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = feature_importance.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 10 Feature Importance (Random Forest)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59445bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of analysis\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n📊 Dataset Information:\")\n",
    "print(f\"   • Original shape: {df.shape}\")\n",
    "print(f\"   • Final shape: {df_features.shape}\")\n",
    "print(f\"   • Features used for modeling: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\n🧹 Data Cleaning:\")\n",
    "print(f\"   • Missing values handled: ✓\")\n",
    "print(f\"   • Duplicates removed: ✓\")\n",
    "print(f\"   • Data types optimized: ✓\")\n",
    "\n",
    "print(f\"\\n🔧 Feature Engineering:\")\n",
    "print(f\"   • Datetime features: {'✓' if 'date' in df_clean.columns else '✗'}\")\n",
    "print(f\"   • Categorical encoding: {'✓' if len(categorical_cols) > 0 else '✗'}\")\n",
    "print(f\"   • Interaction features: ✓\")\n",
    "\n",
    "print(f\"\\n🤖 Machine Learning:\")\n",
    "best_model = model_comparison.loc[model_comparison['Test Accuracy'].idxmax(), 'Model']\n",
    "best_accuracy = model_comparison['Test Accuracy'].max()\n",
    "print(f\"   • Best performing model: {best_model}\")\n",
    "print(f\"   • Best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\n📈 Next Steps:\")\n",
    "print(f\"   • Hyperparameter tuning for better performance\")\n",
    "print(f\"   • Try advanced feature engineering techniques\")\n",
    "print(f\"   • Explore other algorithms (XGBoost, Neural Networks)\")\n",
    "print(f\"   • Deploy the best model for production use\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Analysis completed successfully! 🎉\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# Save processed data\n",
    "df_features.to_csv(PROCESSED_DATA_PATH / 'processed_data.csv', index=False)\n",
    "print(f\"Processed data saved to {PROCESSED_DATA_PATH / 'processed_data.csv'}\")\n",
    "\n",
    "# Save model comparison results\n",
    "model_comparison.to_csv(PROCESSED_DATA_PATH / 'model_comparison.csv', index=False)\n",
    "print(f\"Model comparison saved to {PROCESSED_DATA_PATH / 'model_comparison.csv'}\")\n",
    "\n",
    "print(\"\\nAll results saved successfully! 💾\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
